{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "from data_preprocessing import prepare_lstm_data  \n",
    "from model import LSTMWithMLP\n",
    "\n",
    "# Function to prepare LSTM input data\n",
    "def lstm_data(data, scaler):\n",
    "    if data.empty:\n",
    "        print(f\"No data available for transformation. Skipping...\")\n",
    "        return None  # You might want to handle this case differently based on your model's requirements\n",
    "\n",
    "    data = data.copy()  # Make a copy to avoid SettingWithCopyWarning\n",
    "    data['total_trips'] = data['total_trips'].astype('float64')\n",
    "    data.loc[:, 'total_trips'] = scaler.transform(data[['total_trips']])  # Use .loc to modify in place\n",
    "    X_tensor = torch.tensor(data['total_trips'].values, dtype=torch.float32).unsqueeze(-1)\n",
    "    X_lstm = X_tensor.unsqueeze(0)\n",
    "    return X_lstm\n",
    "\n",
    "\n",
    "def inverse_normalize_count(data, scaler):\n",
    "    data = data.numpy().reshape(-1, 1)  # Convert tensor to numpy array and reshape\n",
    "    data = scaler.inverse_transform(data)\n",
    "    data = np.round(data).astype(int)  # Convert to int to avoid dtype warning\n",
    "    return data\n",
    "\n",
    "# Function to load and prepare MLP input data\n",
    "def prepare_mlp_input(df_mlp, current_date, scaler):\n",
    "    # Prepare data selection\n",
    "    daily_data = df_mlp[df_mlp['date'] == current_date]\n",
    "    daily_data['hour'] = df_mlp['date'].dt.hour\n",
    "    daily_data['month'] = df_mlp['date'].dt.month\n",
    "\n",
    "    # Numeric features\n",
    "    numeric_features = ['temperature_2m_max', 'temperature_2m_min', 'CRASH COUNT', 'precipitation_sum (mm)', 'rain_sum (mm)', 'snowfall_sum (cm)', 'wind_speed_10m_max (km/h)']\n",
    "    \n",
    "    df_mlp_numeric = df_mlp[numeric_features].values \n",
    "    df_mlp_numeric = scaler.transform(df_mlp_numeric)\n",
    "\n",
    "    \n",
    "    df_mlp[numeric_features] = df_mlp_numeric\n",
    "\n",
    "\n",
    "    mlp_input = daily_data.drop(columns='date')\n",
    "    mlp_tensor = torch.tensor(mlp_input.values, dtype=torch.float32)\n",
    "    return mlp_tensor\n",
    "\n",
    "# Function to predict demand\n",
    "def predict_demand(X_lstm, mlp_tensor, model, scaler):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pi, sigma, mu = model(X_lstm, mlp_tensor)\n",
    "    mu = inverse_normalize_count(mu, scaler).flatten()\n",
    "    pi = pi.flatten()\n",
    "    sigma = sigma.flatten()\n",
    "    output = pd.DataFrame({\n",
    "        'demand': mu,\n",
    "        'probability': pi,\n",
    "        'volatility': sigma\n",
    "    })\n",
    "    max_demand = output.loc[output['probability'].idxmax(), 'demand']\n",
    "    return max_demand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total windows created: 1\n",
      "Total windows created: 2\n",
      "Total windows created: 3\n",
      "Total windows created: 4\n",
      "Total windows created: 5\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTMWithMLP:\n\tsize mismatch for lstm.lstm.weight_ih_l0: copying a param with shape torch.Size([512, 1]) from checkpoint, the shape in current model is torch.Size([200, 1]).\n\tsize mismatch for lstm.lstm.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([200, 50]).\n\tsize mismatch for lstm.lstm.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for lstm.lstm.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for lstm.lstm.weight_ih_l1: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([200, 50]).\n\tsize mismatch for lstm.lstm.weight_hh_l1: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([200, 50]).\n\tsize mismatch for lstm.lstm.bias_ih_l1: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for lstm.lstm.bias_hh_l1: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for mlp.fc1.weight: copying a param with shape torch.Size([8, 11]) from checkpoint, the shape in current model is torch.Size([10, 11]).\n\tsize mismatch for mlp.fc1.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([10]).\n\tsize mismatch for mlp.fc2.weight: copying a param with shape torch.Size([128, 8]) from checkpoint, the shape in current model is torch.Size([50, 10]).\n\tsize mismatch for mlp.fc2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([50]).\n\tsize mismatch for mdn.z_pi.weight: copying a param with shape torch.Size([5, 256]) from checkpoint, the shape in current model is torch.Size([5, 100]).\n\tsize mismatch for mdn.z_sigma.weight: copying a param with shape torch.Size([5, 256]) from checkpoint, the shape in current model is torch.Size([5, 100]).\n\tsize mismatch for mdn.z_mu.weight: copying a param with shape torch.Size([5, 256]) from checkpoint, the shape in current model is torch.Size([5, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMWithMLP(lstm_input_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, output_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_gaussians\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, mlp_input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m11\u001b[39m)\n\u001b[0;32m     20\u001b[0m state_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124myanzh\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mcode_and_data\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m4. Deep learning part\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m处理数据\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mdaily_final_model_state.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 21\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Set the date range for prediction from January 1, 2020, to June 30, 2020\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[1;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[0;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[0;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[0;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[0;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTMWithMLP:\n\tsize mismatch for lstm.lstm.weight_ih_l0: copying a param with shape torch.Size([512, 1]) from checkpoint, the shape in current model is torch.Size([200, 1]).\n\tsize mismatch for lstm.lstm.weight_hh_l0: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([200, 50]).\n\tsize mismatch for lstm.lstm.bias_ih_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for lstm.lstm.bias_hh_l0: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for lstm.lstm.weight_ih_l1: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([200, 50]).\n\tsize mismatch for lstm.lstm.weight_hh_l1: copying a param with shape torch.Size([512, 128]) from checkpoint, the shape in current model is torch.Size([200, 50]).\n\tsize mismatch for lstm.lstm.bias_ih_l1: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for lstm.lstm.bias_hh_l1: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for mlp.fc1.weight: copying a param with shape torch.Size([8, 11]) from checkpoint, the shape in current model is torch.Size([10, 11]).\n\tsize mismatch for mlp.fc1.bias: copying a param with shape torch.Size([8]) from checkpoint, the shape in current model is torch.Size([10]).\n\tsize mismatch for mlp.fc2.weight: copying a param with shape torch.Size([128, 8]) from checkpoint, the shape in current model is torch.Size([50, 10]).\n\tsize mismatch for mlp.fc2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([50]).\n\tsize mismatch for mdn.z_pi.weight: copying a param with shape torch.Size([5, 256]) from checkpoint, the shape in current model is torch.Size([5, 100]).\n\tsize mismatch for mdn.z_sigma.weight: copying a param with shape torch.Size([5, 256]) from checkpoint, the shape in current model is torch.Size([5, 100]).\n\tsize mismatch for mdn.z_mu.weight: copying a param with shape torch.Size([5, 256]) from checkpoint, the shape in current model is torch.Size([5, 100])."
     ]
    }
   ],
   "source": [
    "# Load data and preprocess\n",
    "path = r\"C:\\Users\\yanzh\\Desktop\\code_and_data\\4. Deep learning part\\处理数据\\daily\\2015-2019(daily_total_trips).csv\"\n",
    "df1 = pd.read_csv(path)\n",
    "df1['date'] = pd.to_datetime(df1['date'])\n",
    "initial_data = df1[df1['date'] >= pd.to_datetime('2019-12-25')]\n",
    "\n",
    "mlp_path = r\"C:\\Users\\yanzh\\Desktop\\code_and_data\\4. Deep learning part\\处理数据\\daily\\2020mlp.csv\"\n",
    "df_mlp = pd.read_csv(mlp_path)\n",
    "df_mlp['date'] = pd.to_datetime(df_mlp['date'])\n",
    "\n",
    "# Load scalers\n",
    "with open(r\"C:\\Users\\yanzh\\Desktop\\code_and_data\\4. Deep learning part\\处理数据\\daily_scaler1.pkl\", 'rb') as file:\n",
    "    loaded_scaler = pickle.load(file)\n",
    "\n",
    "# Prepare LSTM scaler\n",
    "_, lstm_scaler = prepare_lstm_data()\n",
    "\n",
    "# Load the LSTM+MLP model\n",
    "model = LSTMWithMLP(lstm_input_size=1, output_size=1, num_gaussians=5, mlp_input_dim=11)\n",
    "state_path = r\"C:\\Users\\yanzh\\Desktop\\code_and_data\\4. Deep learning part\\处理数据\\daily_final_model_state.pth\"\n",
    "model.load_state_dict(torch.load(state_path))\n",
    "\n",
    "results = []\n",
    "\n",
    "# Set the date range for prediction from January 1, 2020, to June 30, 2020\n",
    "date_range = pd.date_range(start='2020-01-01', end='2020-06-30')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-08 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-09 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-10 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-11 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-12 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-13 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-14 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-15 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-16 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-17 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-18 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-19 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-20 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-21 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-22 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-23 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-24 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-25 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-26 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-27 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-28 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-29 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-30 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-01-31 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-01 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-02 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-03 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-04 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-05 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-06 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-07 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-08 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-09 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-10 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-11 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-12 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-13 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-14 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-15 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-16 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-17 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-18 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-19 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-20 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-21 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-22 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-23 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-24 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-25 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-26 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-27 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-28 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-02-29 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-01 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-02 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-03 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-04 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-05 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-06 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-07 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-08 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-09 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-10 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-11 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-12 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-13 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-14 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-15 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-16 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-17 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-18 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-19 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-20 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-21 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-22 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-23 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-24 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-25 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-26 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-27 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-28 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-29 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-30 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-03-31 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-01 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-02 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-03 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-04 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-05 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-06 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-07 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-08 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-09 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-10 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-11 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-12 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-13 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-14 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-15 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-16 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-17 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-18 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-19 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-20 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-21 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-22 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-23 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-24 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-25 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-26 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-27 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-28 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-29 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-04-30 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-01 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-02 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-03 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-04 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-05 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-06 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-07 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-08 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-09 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-10 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-11 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-12 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-13 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-14 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-15 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-16 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-17 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-18 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-19 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-20 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-21 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-22 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-23 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-24 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-25 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-26 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-27 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-28 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-29 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-30 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-05-31 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-01 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-02 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-03 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-04 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-05 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-06 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-07 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-08 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-09 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-10 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-11 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-12 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-13 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-14 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-15 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-16 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-17 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-18 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-19 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-20 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-21 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-22 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-23 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-24 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-25 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-26 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-27 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-28 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-29 00:00:00 due to no data.\n",
      "No data available for transformation. Skipping...\n",
      "Skipping prediction for 2020-06-30 00:00:00 due to no data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['hour'] = df_mlp['date'].dt.hour\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['month'] = df_mlp['date'].dt.month\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['hour'] = df_mlp['date'].dt.hour\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['month'] = df_mlp['date'].dt.month\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['hour'] = df_mlp['date'].dt.hour\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['month'] = df_mlp['date'].dt.month\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['hour'] = df_mlp['date'].dt.hour\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['month'] = df_mlp['date'].dt.month\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['hour'] = df_mlp['date'].dt.hour\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['month'] = df_mlp['date'].dt.month\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['hour'] = df_mlp['date'].dt.hour\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['month'] = df_mlp['date'].dt.month\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but MinMaxScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['hour'] = df_mlp['date'].dt.hour\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Temp\\ipykernel_20016\\3619094763.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  daily_data['month'] = df_mlp['date'].dt.month\n",
      "C:\\Users\\yanzh\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but MinMaxScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>predicted_demand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>515863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>129777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>135204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>135946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>136277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>136295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>135448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>117756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-01-02</td>\n",
       "      <td>48929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-01-03</td>\n",
       "      <td>51321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-01-04</td>\n",
       "      <td>50943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-01-05</td>\n",
       "      <td>50246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-01-06</td>\n",
       "      <td>49096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-01-07</td>\n",
       "      <td>47391</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         date  predicted_demand\n",
       "0  2020-01-01            515863\n",
       "1  2020-01-02            129777\n",
       "2  2020-01-03            135204\n",
       "3  2020-01-04            135946\n",
       "4  2020-01-05            136277\n",
       "5  2020-01-06            136295\n",
       "6  2020-01-07            135448\n",
       "7  2020-01-01            117756\n",
       "8  2020-01-02             48929\n",
       "9  2020-01-03             51321\n",
       "10 2020-01-04             50943\n",
       "11 2020-01-05             50246\n",
       "12 2020-01-06             49096\n",
       "13 2020-01-07             47391"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for current_date in date_range:\n",
    "    day_start = current_date - pd.Timedelta(days=7)\n",
    "    data = df1[(df1['date'] >= day_start) & (df1['date'] < current_date)]\n",
    "\n",
    "    X_lstm = lstm_data(data, lstm_scaler)\n",
    "    if X_lstm is None:\n",
    "        print(f\"Skipping prediction for {current_date} due to no data.\")\n",
    "        continue  # Skip this date or handle differently\n",
    "\n",
    "    mlp_tensor = prepare_mlp_input(df_mlp, current_date, loaded_scaler)\n",
    "    daily_demand = predict_demand(X_lstm, mlp_tensor, model, lstm_scaler)\n",
    "    \n",
    "    results.append({\n",
    "        'date': current_date,\n",
    "        'predicted_demand': np.ceil(daily_demand).astype(int)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Assign the traffic demand in specific route"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, given the total demand, we need calculate the probability of each route in specific route.\n",
    "\n",
    "We  will analyze the data from the first hour of January 1st for each year from 2015 to 2019, then get the probability of each specific scenario we defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r\"C:\\Users\\yanzh\\Desktop\\code_and_data\\4. Deep learning part\\trip_counts_with_probability.csv\"\n",
    "data = pd.read_csv(path)\n",
    "\n",
    "data['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
